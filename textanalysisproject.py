# -*- coding: utf-8 -*-
"""TextAnalysisProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mj6ds-F7ogRIQn4do2UL9Bw0I9HPwQJ-
"""

!pip install syllapy

!grep -r "punkt_tab" .

import os
import re
import pandas as pd
import requests
from bs4 import BeautifulSoup
import syllapy
import nltk

try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')


from nltk.tokenize import sent_tokenize, word_tokenize

def load_dictionary(file_path):
    with open(file_path, 'r', encoding='latin1') as file:
        return set([line.strip().lower() for line in file if line.strip()])

positive_words = load_dictionary("positive-words.txt")
negative_words = load_dictionary("negative-words.txt")

def load_stopwords():
    stopwords = set()

    for file in os.listdir():
        if file.lower().startswith("stopwords") and file.lower().endswith(".txt"):
            with open(file, "r", encoding="ISO-8859-1") as f:
                stopwords.update([word.strip().lower() for word in f.readlines()])
    return stopwords

stopwords = load_stopwords()


def clean_text(text):
    return re.sub(r'\s+', ' ', text)

def get_text_from_url(url):
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.content, "html.parser")
        title = soup.find("h1") or soup.title
        paragraphs = soup.find_all("p")
        body_text = ' '.join(p.get_text() for p in paragraphs)
        return title.text.strip(), body_text.strip()
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return "", ""

def save_article(url_id, title, text):
    with open(f"articles/{url_id}.txt", "w", encoding="utf-8") as f:
        f.write(title + "\n" + text)

def count_complex_words(words):
    return sum(1 for word in words if syllapy.count(word) >= 3)

def calculate_variables(text):
    sentences = sent_tokenize(text)
    words = [word for word in word_tokenize(text) if word.isalpha()]
    filtered_words = [word.lower() for word in words if word.lower() not in stopwords]

    pos_score = sum(1 for word in filtered_words if word in positive_words)
    neg_score = sum(1 for word in filtered_words if word in negative_words)
    polarity_score = (pos_score - neg_score) / ((pos_score + neg_score) + 0.000001)
    subjectivity_score = (pos_score + neg_score) / (len(filtered_words) + 0.000001)


    complex_words_count = count_complex_words(filtered_words)
    total_words = len(filtered_words)
    avg_sentence_length = total_words / len(sentences) if sentences else 0
    percentage_complex_words = complex_words_count / total_words if total_words else 0
    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)


    syllable_count = sum(syllapy.count(word) for word in filtered_words)
    syllable_per_word = syllable_count / total_words if total_words else 0
    avg_word_length = sum(len(word) for word in filtered_words) / total_words if total_words else 0
    personal_pronouns = len(re.findall(r'\b(I|we|my|ours|us)\b', text, re.I))

    return [
        pos_score, neg_score, polarity_score, subjectivity_score,
        avg_sentence_length, percentage_complex_words, fog_index,
        avg_sentence_length, complex_words_count, total_words,
        syllable_per_word, personal_pronouns, avg_word_length
    ]


def main():
    os.makedirs("articles", exist_ok=True)

    df_input = pd.read_excel("Input.xlsx")
    output_data = []

    for index, row in df_input.iterrows():
        url_id = row['URL_ID']
        url = row['URL']

        print(f"Processing {url_id}...")

        title, text = get_text_from_url(url)
        if not text:
            continue

        full_text = clean_text(title + " " + text)
        save_article(url_id, title, text)

        analysis = calculate_variables(full_text)
        output_data.append([row[col] for col in df_input.columns] + analysis)

    columns = list(df_input.columns) + [
        'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE',
        'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX',
        'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT',
        'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'
    ]
    df_output = pd.DataFrame(output_data, columns=columns)
    df_output.to_excel("Output.xlsx", index=False)
    print("Processing completed. Output saved to Output.xlsx.")

if __name__ == "__main__":
    main()